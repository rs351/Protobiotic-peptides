{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rs351/Protobiotic-peptides/blob/main/MPNN_2_alphafold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHbETg51UXr0"
      },
      "outputs": [],
      "source": [
        "import json, time, glob\n",
        "import os\n",
        "import torch\n",
        "import shutil\n",
        "import warnings\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import os.path\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split, Subset\n",
        "import re\n",
        "from google.colab import files\n",
        "import hashlib\n",
        "!pip install biopython\n",
        "\n",
        "def run_protein_pipeline(pdb, omit_AAs, run_index):\n",
        "\n",
        "    print(f\"Processing PDB: {pdb}, omit_AAs: {omit_AAs}, Run: {run_index}\")\n",
        "\n",
        "    import os, sys\n",
        "\n",
        "    if not os.path.isdir(\"ProteinMPNN\"):\n",
        "      os.system(\"git clone -q https://github.com/dauparas/ProteinMPNN.git\")\n",
        "    sys.path.append('/content/ProteinMPNN')\n",
        "\n",
        "    import os.path\n",
        "    from protein_mpnn_utils import loss_nll, loss_smoothed, gather_edges, gather_nodes, gather_nodes_t, cat_neighbors_nodes, _scores, _S_to_seq, tied_featurize, parse_PDB\n",
        "    from protein_mpnn_utils import StructureDataset, StructureDatasetPDB, ProteinMPNN\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
        "    #v_48_010=version with 48 edges 0.10A noise\n",
        "    model_name = \"v_48_020\"\n",
        "\n",
        "    backbone_noise=0.00               # Standard deviation of Gaussian noise to add to backbone atoms\n",
        "\n",
        "    path_to_model_weights='/content/ProteinMPNN/vanilla_model_weights'\n",
        "    hidden_dim = 128\n",
        "    num_layers = 3\n",
        "    model_folder_path = path_to_model_weights\n",
        "    if model_folder_path[-1] != '/':\n",
        "        model_folder_path = model_folder_path + '/'\n",
        "    checkpoint_path = model_folder_path + f'{model_name}.pt'\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    print('Number of edges:', checkpoint['num_edges'])\n",
        "    noise_level_print = checkpoint['noise_level']\n",
        "    print(f'Training noise level: {noise_level_print}A')\n",
        "    model = ProteinMPNN(num_letters=21, node_features=hidden_dim, edge_features=hidden_dim, hidden_dim=hidden_dim, num_encoder_layers=num_layers, num_decoder_layers=num_layers, augment_eps=backbone_noise, k_neighbors=checkpoint['num_edges'])\n",
        "    model.to(device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    print(\"Model loaded\")\n",
        "\n",
        "    def make_tied_positions_for_homomers(pdb_dict_list):\n",
        "        my_dict = {}\n",
        "        for result in pdb_dict_list:\n",
        "            all_chain_list = sorted([item[-1:] for item in list(result) if item[:9]=='seq_chain']) #A, B, C, ...\n",
        "            tied_positions_list = []\n",
        "            chain_length = len(result[f\"seq_chain_{all_chain_list[0]}\"])\n",
        "            for i in range(1,chain_length+1):\n",
        "                temp_dict = {}\n",
        "                for j, chain in enumerate(all_chain_list):\n",
        "                    temp_dict[chain] = [i] #needs to be a list\n",
        "                tied_positions_list.append(temp_dict)\n",
        "            my_dict[result['name']] = tied_positions_list\n",
        "        return my_dict\n",
        "\n",
        "    import re\n",
        "    import numpy as np\n",
        "\n",
        "    def get_pdb(pdb_code=\"\"):\n",
        "      if pdb_code is None or pdb_code == \"\":\n",
        "        upload_dict = files.upload()\n",
        "        pdb_string = upload_dict[list(upload_dict.keys())[0]]\n",
        "        with open(\"tmp.pdb\",\"wb\") as out: out.write(pdb_string)\n",
        "        return \"tmp.pdb\"\n",
        "      else:\n",
        "        os.system(f\"wget -qnc https://files.rcsb.org/view/{pdb_code}.pdb\")\n",
        "        return f\"{pdb_code}.pdb\"\n",
        "\n",
        "    pdb_path = get_pdb(pdb)\n",
        "\n",
        "    homomer = True\n",
        "    designed_chain = \"A\"\n",
        "    fixed_chain = \"\"\n",
        "\n",
        "    if designed_chain == \"\":\n",
        "      designed_chain_list = []\n",
        "    else:\n",
        "      designed_chain_list = re.sub(\"[^A-Za-z]+\",\",\", designed_chain).split(\",\")\n",
        "\n",
        "    if fixed_chain == \"\":\n",
        "      fixed_chain_list = []\n",
        "    else:\n",
        "      fixed_chain_list = re.sub(\"[^A-Za-z]+\",\",\", fixed_chain).split(\",\")\n",
        "\n",
        "    chain_list = list(set(designed_chain_list + fixed_chain_list))\n",
        "\n",
        "    num_seqs = 1\n",
        "    num_seq_per_target = num_seqs\n",
        "\n",
        "    sampling_temp = \"0.1\"\n",
        "\n",
        "    save_score=0                      # 0 for False, 1 for True; save score=-log_prob to npy files\n",
        "    save_probs=0                      # 0 for False, 1 for True; save MPNN predicted probabilites per position\n",
        "    score_only=0                      # 0 for False, 1 for True; score input backbone-sequence pairs\n",
        "    conditional_probs_only=0          # 0 for False, 1 for True; output conditional probabilities p(s_i given the rest of the sequence and backbone)\n",
        "    conditional_probs_only_backbone=0 # 0 for False, 1 for True; if true output conditional probabilities p(s_i given backbone)\n",
        "\n",
        "    batch_size=1                      # Batch size; can set higher for titan, quadro GPUs, reduce this if running out of GPU memory\n",
        "    max_length=20000                  # Max sequence length\n",
        "\n",
        "    out_folder='.'                    # Path to a folder to output sequences, e.g. /home/out/\n",
        "    jsonl_path=''                     # Path to a folder with parsed pdb into jsonl\n",
        "    # omit_AAs='X'                      # Specify which amino acids should be omitted in the generated sequence, e.g. 'AC' would omit alanine and cystine.\n",
        "\n",
        "    pssm_multi=0.0                    # A value between [0.0, 1.0], 0.0 means do not use pssm, 1.0 ignore MPNN predictions\n",
        "    pssm_threshold=0.0                # A value between -inf + inf to restric per position AAs\n",
        "    pssm_log_odds_flag=0               # 0 for False, 1 for True\n",
        "    pssm_bias_flag=0                   # 0 for False, 1 for True\n",
        "\n",
        "    ##############################################################\n",
        "\n",
        "    folder_for_outputs = out_folder\n",
        "\n",
        "    NUM_BATCHES = num_seq_per_target//batch_size\n",
        "    BATCH_COPIES = batch_size\n",
        "    temperatures = [float(item) for item in sampling_temp.split()]\n",
        "    omit_AAs_list = omit_AAs\n",
        "    alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
        "\n",
        "    omit_AAs_np = np.array([AA in omit_AAs_list for AA in alphabet]).astype(np.float32)\n",
        "\n",
        "    chain_id_dict = None\n",
        "    fixed_positions_dict = None\n",
        "    pssm_dict = None\n",
        "    omit_AA_dict = None\n",
        "    bias_AA_dict = None\n",
        "    tied_positions_dict = None\n",
        "    bias_by_res_dict = None\n",
        "    bias_AAs_np = np.zeros(len(alphabet))\n",
        "\n",
        "    ###############################################################\n",
        "    pdb_dict_list = parse_PDB(pdb_path, input_chain_list=chain_list)\n",
        "    dataset_valid = StructureDatasetPDB(pdb_dict_list, truncate=None, max_length=max_length)\n",
        "\n",
        "    chain_id_dict = {}\n",
        "    chain_id_dict[pdb_dict_list[0]['name']]= (designed_chain_list, fixed_chain_list)\n",
        "\n",
        "    print(chain_id_dict)\n",
        "    for chain in chain_list:\n",
        "      l = len(pdb_dict_list[0][f\"seq_chain_{chain}\"])\n",
        "      print(f\"Length of chain {chain} is {l}\")\n",
        "\n",
        "    if homomer:\n",
        "      tied_positions_dict = make_tied_positions_for_homomers(pdb_dict_list)\n",
        "    else:\n",
        "      tied_positions_dict = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "      print('Generating sequences...')\n",
        "      for ix, protein in enumerate(dataset_valid):\n",
        "        score_list = []\n",
        "        all_probs_list = []\n",
        "        all_log_probs_list = []\n",
        "        S_sample_list = []\n",
        "        batch_clones = [copy.deepcopy(protein) for i in range(BATCH_COPIES)]\n",
        "        X, S, mask, lengths, chain_M, chain_encoding_all, chain_list_list, visible_list_list, masked_list_list, masked_chain_length_list_list, chain_M_pos, omit_AA_mask, residue_idx, dihedral_mask, tied_pos_list_of_lists_list, pssm_coef, pssm_bias, pssm_log_odds_all, bias_by_res_all, tied_beta = tied_featurize(batch_clones, device, chain_id_dict, fixed_positions_dict, omit_AA_dict, tied_positions_dict, pssm_dict, bias_by_res_dict)\n",
        "        pssm_log_odds_mask = (pssm_log_odds_all > pssm_threshold).float() #1.0 for true, 0.0 for false\n",
        "        name_ = batch_clones[0]['name']\n",
        "\n",
        "        randn_1 = torch.randn(chain_M.shape, device=X.device)\n",
        "        log_probs = model(X, S, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_1)\n",
        "        mask_for_loss = mask*chain_M*chain_M_pos\n",
        "        scores = _scores(S, log_probs, mask_for_loss)\n",
        "        native_score = scores.cpu().data.numpy()\n",
        "\n",
        "        for temp in temperatures:\n",
        "            for j in range(NUM_BATCHES):\n",
        "                randn_2 = torch.randn(chain_M.shape, device=X.device)\n",
        "                if tied_positions_dict == None:\n",
        "                    sample_dict = model.sample(X, randn_2, S, chain_M, chain_encoding_all, residue_idx, mask=mask, temperature=temp, omit_AAs_np=omit_AAs_np, bias_AAs_np=bias_AAs_np, chain_M_pos=chain_M_pos, omit_AA_mask=omit_AA_mask, pssm_coef=pssm_coef, pssm_bias=pssm_bias, pssm_multi=pssm_multi, pssm_log_odds_flag=bool(pssm_log_odds_flag), pssm_log_odds_mask=pssm_log_odds_mask, pssm_bias_flag=bool(pssm_bias_flag), bias_by_res=bias_by_res_all)\n",
        "                    S_sample = sample_dict[\"S\"]\n",
        "                else:\n",
        "                    sample_dict = model.tied_sample(X, randn_2, S, chain_M, chain_encoding_all, residue_idx, mask=mask, temperature=temp, omit_AAs_np=omit_AAs_np, bias_AAs_np=bias_AAs_np, chain_M_pos=chain_M_pos, omit_AA_mask=omit_AA_mask, pssm_coef=pssm_coef, pssm_bias=pssm_bias, pssm_multi=pssm_multi, pssm_log_odds_flag=bool(pssm_log_odds_flag), pssm_log_odds_mask=pssm_log_odds_mask, pssm_bias_flag=bool(pssm_bias_flag), tied_pos=tied_pos_list_of_lists_list[0], tied_beta=tied_beta, bias_by_res=bias_by_res_all)\n",
        "                # Compute scores\n",
        "                    S_sample = sample_dict[\"S\"]\n",
        "                log_probs = model(X, S_sample, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_2, use_input_decoding_order=True, decoding_order=sample_dict[\"decoding_order\"])\n",
        "                mask_for_loss = mask*chain_M*chain_M_pos\n",
        "                scores = _scores(S_sample, log_probs, mask_for_loss)\n",
        "                scores = scores.cpu().data.numpy()\n",
        "                all_probs_list.append(sample_dict[\"probs\"].cpu().data.numpy())\n",
        "                all_log_probs_list.append(log_probs.cpu().data.numpy())\n",
        "                S_sample_list.append(S_sample.cpu().data.numpy())\n",
        "                for b_ix in range(BATCH_COPIES):\n",
        "                    masked_chain_length_list = masked_chain_length_list_list[b_ix]\n",
        "                    masked_list = masked_list_list[b_ix]\n",
        "                    seq_recovery_rate = torch.sum(torch.sum(torch.nn.functional.one_hot(S[b_ix], 21)*torch.nn.functional.one_hot(S_sample[b_ix], 21),axis=-1)*mask_for_loss[b_ix])/torch.sum(mask_for_loss[b_ix])\n",
        "                    seq = _S_to_seq(S_sample[b_ix], chain_M[b_ix])\n",
        "                    score = scores[b_ix]\n",
        "                    score_list.append(score)\n",
        "                    native_seq = _S_to_seq(S[b_ix], chain_M[b_ix])\n",
        "                    if b_ix == 0 and j==0 and temp==temperatures[0]:\n",
        "                        start = 0\n",
        "                        end = 0\n",
        "                        list_of_AAs = []\n",
        "                        for mask_l in masked_chain_length_list:\n",
        "                            end += mask_l\n",
        "                            list_of_AAs.append(native_seq[start:end])\n",
        "                            start = end\n",
        "                        native_seq = \"\".join(list(np.array(list_of_AAs)[np.argsort(masked_list)]))\n",
        "                        l0 = 0\n",
        "                        for mc_length in list(np.array(masked_chain_length_list)[np.argsort(masked_list)])[:-1]:\n",
        "                            l0 += mc_length\n",
        "                            native_seq = native_seq[:l0] + '/' + native_seq[l0:]\n",
        "                            l0 += 1\n",
        "                        sorted_masked_chain_letters = np.argsort(masked_list_list[0])\n",
        "                        print_masked_chains = [masked_list_list[0][i] for i in sorted_masked_chain_letters]\n",
        "                        sorted_visible_chain_letters = np.argsort(visible_list_list[0])\n",
        "                        print_visible_chains = [visible_list_list[0][i] for i in sorted_visible_chain_letters]\n",
        "                        native_score_print = np.format_float_positional(np.float32(native_score.mean()), unique=False, precision=4)\n",
        "                        line = '>{}, score={}, fixed_chains={}, designed_chains={}, model_name={}\\n{}\\n'.format(name_, native_score_print, print_visible_chains, print_masked_chains, model_name, native_seq)\n",
        "                        print(line.rstrip())\n",
        "                    start = 0\n",
        "                    end = 0\n",
        "                    list_of_AAs = []\n",
        "                    for mask_l in masked_chain_length_list:\n",
        "                        end += mask_l\n",
        "                        list_of_AAs.append(seq[start:end])\n",
        "                        start = end\n",
        "\n",
        "                    seq = \"\".join(list(np.array(list_of_AAs)[np.argsort(masked_list)]))\n",
        "                    l0 = 0\n",
        "                    for mc_length in list(np.array(masked_chain_length_list)[np.argsort(masked_list)])[:-1]:\n",
        "                        l0 += mc_length\n",
        "                        seq = seq[:l0] + '/' + seq[l0:]\n",
        "                        l0 += 1\n",
        "                    score_print = np.format_float_positional(np.float32(score), unique=False, precision=4)\n",
        "                    seq_rec_print = np.format_float_positional(np.float32(seq_recovery_rate.detach().cpu().numpy()), unique=False, precision=4)\n",
        "                    line = '>T={}, sample={}, score={}, seq_recovery={}\\n{}\\n'.format(temp,b_ix,score_print,seq_rec_print,seq)\n",
        "                    print(line.rstrip())\n",
        "\n",
        "    all_probs_concat = np.concatenate(all_probs_list)\n",
        "    all_log_probs_concat = np.concatenate(all_log_probs_list)\n",
        "    S_sample_concat = np.concatenate(S_sample_list)\n",
        "\n",
        "    ######################\n",
        "\n",
        "    from sys import version_info\n",
        "    python_version = f\"{version_info.major}.{version_info.minor}\"\n",
        "\n",
        "    def add_hash(x,y):\n",
        "      return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n",
        "\n",
        "    query_sequence = seq\n",
        "\n",
        "    jobname = 'test'\n",
        "    # number of models to use\n",
        "    num_relax = 0\n",
        "    template_mode = \"none\"\n",
        "\n",
        "    use_amber = num_relax > 0\n",
        "\n",
        "    # remove whitespaces\n",
        "    query_sequence = \"\".join(query_sequence.split())\n",
        "\n",
        "    basejobname = \"\".join(jobname.split())\n",
        "    basejobname = re.sub(r'\\W+', '', basejobname)\n",
        "    jobname = add_hash(basejobname, query_sequence)\n",
        "    jobname = f\"{pdb}.{omit_AAs[:-1]}.{run_index}\"\n",
        "\n",
        "    # check if directory with jobname exists\n",
        "    def check(folder):\n",
        "        return not os.path.exists(folder)\n",
        "\n",
        "    if not check(jobname):\n",
        "        # If the folder exists, remove it and all its contents\n",
        "        shutil.rmtree(jobname)\n",
        "\n",
        "    # make directory to save results\n",
        "    os.makedirs(jobname, exist_ok=True)\n",
        "\n",
        "    # save queries\n",
        "    queries_path = os.path.join(jobname, f\"{jobname}.csv\")\n",
        "    with open(queries_path, \"w\") as text_file:\n",
        "      text_file.write(f\"id,sequence\\n{jobname},{query_sequence}\")\n",
        "\n",
        "    if template_mode == \"pdb100\":\n",
        "      use_templates = True\n",
        "      custom_template_path = None\n",
        "    elif template_mode == \"custom\":\n",
        "      custom_template_path = os.path.join(jobname,f\"template\")\n",
        "      os.makedirs(custom_template_path, exist_ok=True)\n",
        "      uploaded = files.upload()\n",
        "      use_templates = True\n",
        "      for fn in uploaded.keys():\n",
        "        os.rename(fn,os.path.join(custom_template_path,fn))\n",
        "    else:\n",
        "      custom_template_path = None\n",
        "      use_templates = False\n",
        "\n",
        "    print(\"jobname\",jobname)\n",
        "    print(\"sequence\",query_sequence)\n",
        "    print(\"length\",len(query_sequence.replace(\":\",\"\")))\n",
        "\n",
        "    import os\n",
        "    USE_AMBER = use_amber\n",
        "    USE_TEMPLATES = use_templates\n",
        "    PYTHON_VERSION = python_version\n",
        "\n",
        "    if not os.path.isfile(\"COLABFOLD_READY\"):\n",
        "      print(\"installing colabfold...\")\n",
        "      os.system(\"pip install -q --no-warn-conflicts 'colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold'\")\n",
        "      if os.environ.get('TPU_NAME', False) != False:\n",
        "        os.system(\"pip uninstall -y jax jaxlib\")\n",
        "        os.system(\"pip install --no-warn-conflicts --upgrade dm-haiku==0.0.10 'jax[cuda12_pip]'==0.3.25 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\")\n",
        "      os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold\")\n",
        "      os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold\")\n",
        "      os.system(\"touch COLABFOLD_READY\")\n",
        "\n",
        "    if USE_AMBER or USE_TEMPLATES:\n",
        "      if not os.path.isfile(\"CONDA_READY\"):\n",
        "        print(\"installing conda...\")\n",
        "        os.system(\"wget -qnc https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\")\n",
        "        os.system(\"bash Miniforge3-Linux-x86_64.sh -bfp /usr/local\")\n",
        "        os.system(\"mamba config --set auto_update_conda false\")\n",
        "        os.system(\"touch CONDA_READY\")\n",
        "\n",
        "    if USE_TEMPLATES and not os.path.isfile(\"HH_READY\") and USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n",
        "      print(\"installing hhsuite and amber...\")\n",
        "      os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 openmm=7.7.0 python='{PYTHON_VERSION}' pdbfixer\")\n",
        "      os.system(\"touch HH_READY\")\n",
        "      os.system(\"touch AMBER_READY\")\n",
        "    else:\n",
        "      if USE_TEMPLATES and not os.path.isfile(\"HH_READY\"):\n",
        "        print(\"installing hhsuite...\")\n",
        "        os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python='{PYTHON_VERSION}'\")\n",
        "        os.system(\"touch HH_READY\")\n",
        "      if USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n",
        "        print(\"installing amber...\")\n",
        "        os.system(f\"mamba install -y -c conda-forge openmm=7.7.0 python='{PYTHON_VERSION}' pdbfixer\")\n",
        "        os.system(\"touch AMBER_READY\")\n",
        "\n",
        "    msa_mode = \"mmseqs2_uniref_env\"\n",
        "    pair_mode = \"unpaired_paired\"\n",
        "\n",
        "    # decide which a3m to use\n",
        "    if \"mmseqs2\" in msa_mode:\n",
        "      a3m_file = os.path.join(jobname,f\"{jobname}.a3m\")\n",
        "\n",
        "    elif msa_mode == \"custom\":\n",
        "      a3m_file = os.path.join(jobname,f\"{jobname}.custom.a3m\")\n",
        "      if not os.path.isfile(a3m_file):\n",
        "        custom_msa_dict = files.upload()\n",
        "        custom_msa = list(custom_msa_dict.keys())[0]\n",
        "        header = 0\n",
        "        import fileinput\n",
        "        for line in fileinput.FileInput(custom_msa,inplace=1):\n",
        "          if line.startswith(\">\"):\n",
        "            header = header + 1\n",
        "          if not line.rstrip():\n",
        "            continue\n",
        "          if line.startswith(\">\") == False and header == 1:\n",
        "            query_sequence = line.rstrip()\n",
        "          print(line, end='')\n",
        "\n",
        "        os.rename(custom_msa, a3m_file)\n",
        "        queries_path=a3m_file\n",
        "        print(f\"moving {custom_msa} to {a3m_file}\")\n",
        "\n",
        "    else:\n",
        "      a3m_file = os.path.join(jobname,f\"{jobname}.single_sequence.a3m\")\n",
        "      with open(a3m_file, \"w\") as text_file:\n",
        "        text_file.write(\">1\\n%s\" % query_sequence)\n",
        "\n",
        "    model_type = \"auto\"\n",
        "    num_recycles = \"3\"\n",
        "    recycle_early_stop_tolerance = \"auto\"\n",
        "    relax_max_iterations = 200\n",
        "    pairing_strategy = \"greedy\"\n",
        "\n",
        "    max_msa = \"auto\"\n",
        "    num_seeds = 1\n",
        "    use_dropout = False\n",
        "\n",
        "    num_recycles = None if num_recycles == \"auto\" else int(num_recycles)\n",
        "    recycle_early_stop_tolerance = None if recycle_early_stop_tolerance == \"auto\" else float(recycle_early_stop_tolerance)\n",
        "    if max_msa == \"auto\": max_msa = None\n",
        "\n",
        "    save_all = False\n",
        "    save_recycles = False\n",
        "    save_to_google_drive = True\n",
        "    dpi = 200\n",
        "\n",
        "    if save_to_google_drive:\n",
        "      from pydrive2.drive import GoogleDrive\n",
        "      from pydrive2.auth import GoogleAuth\n",
        "      from google.colab import auth\n",
        "      from oauth2client.client import GoogleCredentials\n",
        "      auth.authenticate_user()\n",
        "      gauth = GoogleAuth()\n",
        "      gauth.credentials = GoogleCredentials.get_application_default()\n",
        "      drive = GoogleDrive(gauth)\n",
        "      print(\"You are logged into Google Drive and are good to go!\")\n",
        "\n",
        "    display_images = False\n",
        "\n",
        "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "    from Bio import BiopythonDeprecationWarning\n",
        "    warnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)\n",
        "    from pathlib import Path\n",
        "    from colabfold.download import download_alphafold_params, default_data_dir\n",
        "    from colabfold.utils import setup_logging\n",
        "    from colabfold.batch import get_queries, run, set_model_type\n",
        "    from colabfold.plot import plot_msa_v2\n",
        "\n",
        "    try:\n",
        "      K80_chk = os.popen('nvidia-smi | grep \"Tesla K80\" | wc -l').read()\n",
        "    except:\n",
        "      K80_chk = \"0\"\n",
        "      pass\n",
        "    if \"1\" in K80_chk:\n",
        "      print(\"WARNING: found GPU Tesla K80: limited to total length < 1000\")\n",
        "      if \"TF_FORCE_UNIFIED_MEMORY\" in os.environ:\n",
        "        del os.environ[\"TF_FORCE_UNIFIED_MEMORY\"]\n",
        "      if \"XLA_PYTHON_CLIENT_MEM_FRACTION\" in os.environ:\n",
        "        del os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]\n",
        "\n",
        "    from colabfold.colabfold import plot_protein\n",
        "    from pathlib import Path\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # For some reason we need that to get pdbfixer to import\n",
        "    if use_amber and f\"/usr/local/lib/python{python_version}/site-packages/\" not in sys.path:\n",
        "        sys.path.insert(0, f\"/usr/local/lib/python{python_version}/site-packages/\")\n",
        "\n",
        "    def input_features_callback(input_features):\n",
        "      if display_images:\n",
        "        plot_msa_v2(input_features)\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    def prediction_callback(protein_obj, length,\n",
        "                            prediction_result, input_features, mode):\n",
        "      model_name, relaxed = mode\n",
        "      if not relaxed:\n",
        "        if display_images:\n",
        "          fig = plot_protein(protein_obj, Ls=length, dpi=150)\n",
        "          plt.show()\n",
        "          plt.close()\n",
        "\n",
        "    result_dir = jobname\n",
        "    log_filename = os.path.join(jobname,\"log.txt\")\n",
        "    setup_logging(Path(log_filename))\n",
        "\n",
        "    queries, is_complex = get_queries(queries_path)\n",
        "    model_type = set_model_type(is_complex, model_type)\n",
        "\n",
        "    if \"multimer\" in model_type and max_msa is not None:\n",
        "      use_cluster_profile = False\n",
        "    else:\n",
        "      use_cluster_profile = True\n",
        "\n",
        "    download_alphafold_params(model_type, Path(\".\"))\n",
        "    results = run(\n",
        "        queries=queries,\n",
        "        result_dir=result_dir,\n",
        "        use_templates=use_templates,\n",
        "        custom_template_path=custom_template_path,\n",
        "        num_relax=num_relax,\n",
        "        msa_mode=msa_mode,\n",
        "        model_type=model_type,\n",
        "        num_models=1,\n",
        "        num_recycles=num_recycles,\n",
        "        relax_max_iterations=relax_max_iterations,\n",
        "        recycle_early_stop_tolerance=recycle_early_stop_tolerance,\n",
        "        num_seeds=num_seeds,\n",
        "        use_dropout=use_dropout,\n",
        "        model_order=[1,2,3,4,5],\n",
        "        is_complex=is_complex,\n",
        "        data_dir=Path(\".\"),\n",
        "        keep_existing_results=False,\n",
        "        rank_by=\"auto\",\n",
        "        pair_mode=pair_mode,\n",
        "        pairing_strategy=pairing_strategy,\n",
        "        stop_at_score=float(100),\n",
        "        prediction_callback=prediction_callback,\n",
        "        dpi=dpi,\n",
        "        zip_results=False,\n",
        "        save_all=save_all,\n",
        "        max_msa=max_msa,\n",
        "        use_cluster_profile=use_cluster_profile,\n",
        "        input_features_callback=input_features_callback,\n",
        "        save_recycles=save_recycles,\n",
        "        user_agent=\"colabfold/google-colab-main\",\n",
        "    )\n",
        "\n",
        "    if msa_mode == \"custom\":\n",
        "      print(\"Don't forget to cite your custom MSA generation method.\")\n",
        "\n",
        "    results_zip = f\"{jobname}.result.zip\"\n",
        "    os.system(f\"zip -r {results_zip} {jobname}\")\n",
        "    # files.download(f\"{jobname}.result.zip\")\n",
        "\n",
        "    if save_to_google_drive == True and drive:\n",
        "      uploaded = drive.CreateFile({'title': f\"{pdb}.{omit_AAs[:-1]}.{run_index}.zip\"})\n",
        "      uploaded.SetContentFile(f\"{jobname}.result.zip\")\n",
        "      uploaded.Upload()\n",
        "      print(f\"Uploaded {pdb}.{omit_AAs[:-1]}.{run_index}.zip to Google Drive with ID {uploaded.get('id')}\")\n",
        "\n",
        "# Define combinations\n",
        "pdb_list = ['3on3', '5b46', '1aco', '5acn', '1cts', '1vgp', '1h98', '1iqz', '1rfk', '1rgv', '1siz', '1yfe', '7xky', '5glg', '6awf','3icd', '4aou', '1b8p', '1hlp', '2x0i', '7cgd', '2raa', '9bt4', '1euc', '1oi7']\n",
        "\n",
        "# **********Amino acid LIBRARIES************\n",
        "# omit_AAs_list = ['LECDFHIKMNQRSTWYX', 'ECDFHIKMNQRSTWYX', 'CDFHIKMNQRSTWYX'] # Bennu\n",
        "# omit_AAs_list = ['VEFCHIKLMNPQRTWYX', 'EFCHIKLMNPQRTWYX', 'FCHIKLMNPQRTWYX', 'CHIKLMNPQRTWYX'] # Johnson\n",
        "# omit_AAs_list = ['CFHIKLMNPQRVWYX', 'CFHIKLMNPQRWYX', 'CFHIKLMNQRWYX', 'CFHIKMNQRWYX', 'CFHKMNQRWYX'] # Murch\n",
        "# omit_AAs_list = ['TESLICDFHKMNQRWYX', 'ESLICDFHKMNQRWYX', 'SLICDFHKMNQRWYX', 'LICDFHKMNQRWYX', 'ICDFHKMNQRWYX', 'CDFHKMNQRWYX'] # Murchison recent paper reduced\n",
        "omit_AAs_list = ['TIELDMCFHKNPQRWY', 'IELDMCFHKNPQRWYX', 'ELDMCFHKNPQRWYX', 'LDMCFHKNPQRWYX', 'DMCFHKNPQRWYX', 'MCFHKNPQRWYX', 'CFHKNPQRWYX'] # Parker MU\n",
        "\n",
        "combinations = [(pdb, omit_AAs) for pdb in pdb_list for omit_AAs in omit_AAs_list]\n",
        "\n",
        "# Run the pipeline for each combination\n",
        "for pdb, omit_AAs in combinations:\n",
        "    for run_index in range(1, 6):  # 5 repetitions for each combination\n",
        "        run_protein_pipeline(pdb, omit_AAs, run_index)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}